{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from math import pi\n",
    "import einops as einops\n",
    "\n",
    "from training.utils.FLAIR_2 import*\n",
    "import matplotlib.pyplot as plt\n",
    "from training.perceiver import*\n",
    "from training.utils import*\n",
    "from training.losses import*\n",
    "from training.VIT import*\n",
    "from training.ResNet import*\n",
    "from collections import defaultdict\n",
    "from training import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dm = CloudSEN12DataModule(batch_size=8, num_workers=8)\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "tmp_x=None\n",
    "tmp_y=None\n",
    "\n",
    "for x, y in dm.train_dataloader():\n",
    "    tmp_x=x\n",
    "    tmp_y=y\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 384, 384]) torch.Size([8, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "dm = Cloud95DataModule(batch_size=8, num_workers=0)\n",
    "dm.setup(\"test\")\n",
    "\n",
    "for x, y in dm.test_dataloader():\n",
    "    print(x.shape, y.shape)  # Expected: [B, 4, H, W], [B, H, W]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>log train_loss</td><td>▁</td></tr><tr><td>log val_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁</td></tr><tr><td>val_Dice</td><td>▁</td></tr><tr><td>val_IoU</td><td>▁</td></tr><tr><td>val_Precision</td><td>▁</td></tr><tr><td>val_Recall</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>log train_loss</td><td>0.23545</td></tr><tr><td>log val_loss</td><td>0.27232</td></tr><tr><td>train_loss</td><td>1.26547</td></tr><tr><td>trainer/global_step</td><td>1</td></tr><tr><td>val_Dice</td><td>0.17939</td></tr><tr><td>val_IoU</td><td>0.15401</td></tr><tr><td>val_Precision</td><td>0.17893</td></tr><tr><td>val_Recall</td><td>0.23445</td></tr><tr><td>val_loss</td><td>1.313</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Unet</strong> at: <a href='https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet/runs/tm6sz4j5' target=\"_blank\">https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet/runs/tm6sz4j5</a><br> View project at: <a href='https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet' target=\"_blank\">https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250605_140207-tm6sz4j5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hugoriffaud/Documents/Atomiser_CloudDetection/wandb/run-20250605_140727-76eui2e4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet/runs/76eui2e4' target=\"_blank\">Unet</a></strong> to <a href='https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet' target=\"_blank\">https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet/runs/76eui2e4' target=\"_blank\">https://wandb.ai/hugordet-inria/Atomizer_BigEarthNet/runs/76eui2e4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/training/Unet/unet.py:211: FutureWarning: `nn.init.xavier_normal` is now deprecated in favor of `nn.init.xavier_normal_`.\n",
      "  init.xavier_normal(m.weight)\n",
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/training/Unet/unet.py:212: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  init.constant(m.bias, 0)\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/hugoriffaud/Documents/Atomiser_CloudDetection/checkpoints exists and is not empty.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=16). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | metric_IoU       | MulticlassJaccardIndex | 0      | train\n",
      "1 | metric_Dice      | DiceScore              | 0      | train\n",
      "2 | metric_Precision | MulticlassPrecision    | 0      | train\n",
      "3 | metric_Recall    | MulticlassRecall       | 0      | train\n",
      "4 | encoder          | UNet                   | 31.0 M | train\n",
      "5 | loss             | CrossEntropyLoss       | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "31.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 M    Total params\n",
      "124.148   Total estimated model params size (MB)\n",
      "44        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/Atomiser_CloudDetection/venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:24<00:00,  0.08it/s, v_num=i2e4]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(85208) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(85209) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(85210) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(85211) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(85212) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(85213) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(85214) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(85215) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from training.perceiver import *\n",
    "from training.utils import *\n",
    "from training.losses import *\n",
    "\n",
    "from training.VIT import *\n",
    "from training.ResNet import *\n",
    "from collections import defaultdict\n",
    "from training import *\n",
    "import os\n",
    "from pytorch_lightning import Trainer,seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint,GradientAccumulationScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import einops as einops\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Reduce\n",
    "from pytorch_lightning.profilers import AdvancedProfiler\n",
    "profiler = AdvancedProfiler(dirpath=\"./profiling\", filename=\"advanced_profile.txt\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "config_model = read_yaml(\"./training/configs/Unet.yaml\")\n",
    "xp_name=\"testset\"\n",
    "\n",
    "\n",
    "wand = True\n",
    "wandb_logger = None\n",
    "if wand:\n",
    "    if os.environ.get(\"LOCAL_RANK\", \"0\") == \"0\":\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            name=config_model['encoder'],\n",
    "            project=\"Atomizer_BigEarthNet\",\n",
    "            config=config_model\n",
    "        )\n",
    "        wandb_logger = WandbLogger(project=\"Atomizer_Cloud\")\n",
    "\n",
    "#def __init__(self, config, wand, name)\n",
    "model = Model(config_model,wand=wand,transform=None)\n",
    "\n",
    "data_module=CloudSEN12DataModule(batch_size=config_model[\"dataset\"][\"batchsize\"], num_workers=8)\n",
    "# Callbacks\n",
    "checkpoint_callback_IoU = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints/\",\n",
    "    filename=config_model[\"encoder\"]+str(xp_name)+\"-alone_best_model_val_mod_val-{epoch:02d}-{val_mod_val_ap:.4f}\",\n",
    "    monitor=\"val_IoU\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accumulator = GradientAccumulationScheduler(scheduling={0: 64,50:8})\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    #strategy=\"ddp\",\n",
    "    #strategy='ddp_find_unused_parameters_true',\n",
    "    devices=-1, \n",
    "    max_epochs=config_model[\"trainer\"][\"epochs\"],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=16,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[checkpoint_callback_IoU],#,accumulator],\n",
    "    default_root_dir=\"./checkpoints/\",\n",
    "    #val_check_interval=0.3,\n",
    "    precision=\"bf16-mixed\",\n",
    "    profiler=profiler,\n",
    "    limit_test_batches=2,\n",
    "    limit_train_batches=2,\n",
    "    limit_val_batches=2\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ... after training completes, within your \"if wand\" block:\n",
    "if wand and os.environ.get(\"LOCAL_RANK\", \"0\") == \"0\":\n",
    "    run_id = wandb.run.id\n",
    "    print(\"WANDB_RUN_ID:\", run_id)\n",
    "    \n",
    "    # Create the directory for storing wandb run IDs if it doesn't exist\n",
    "    runs_dir = \"training/wandb_runs\"\n",
    "    os.makedirs(runs_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Save the run ID to a file inside wandb_runs\n",
    "    run_file = os.path.join(runs_dir, xp_name+\".txt\")\n",
    "    with open(run_file, \"w\") as f:\n",
    "        f.write(run_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
